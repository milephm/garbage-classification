{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-05-21T11:09:59.228523Z","iopub.execute_input":"2025-05-21T11:09:59.228736Z","iopub.status.idle":"2025-05-21T11:10:34.850675Z","shell.execute_reply.started":"2025-05-21T11:09:59.228718Z","shell.execute_reply":"2025-05-21T11:10:34.849943Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport kagglehub\ncrawford_xception_path = kagglehub.dataset_download('crawford/xception')\nmostafaabla_garbage_classification_path = kagglehub.dataset_download('mostafaabla/garbage-classification')\n\nprint('Data source import complete.')","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:34.851537Z","iopub.execute_input":"2025-05-21T11:10:34.851951Z","iopub.status.idle":"2025-05-21T11:10:35.353514Z","shell.execute_reply.started":"2025-05-21T11:10:34.851923Z","shell.execute_reply":"2025-05-21T11:10:35.352920Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2: Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport random\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras.applications.xception as xception\nimport tensorflow.keras as keras\nimport tensorflow as tf\nimport re\nimport zipfile\nimport sys\nimport time\nfrom PIL import Image\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Activation, Lambda\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_recall_fscore_support\n\nprint('Setup successful!')","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:35.355179Z","iopub.execute_input":"2025-05-21T11:10:35.355352Z","iopub.status.idle":"2025-05-21T11:10:48.917239Z","shell.execute_reply.started":"2025-05-21T11:10:35.355338Z","shell.execute_reply":"2025-05-21T11:10:48.916637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3: Define constants and paths","metadata":{}},{"cell_type":"code","source":"# Define image size - may increase to improve accuracy\nIMAGE_WIDTH = 320\nIMAGE_HEIGHT = 320\nIMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\nIMAGE_CHANNELS = 3\n\n# Path to dataset\nbase_path = \"../input/garbage-classification/garbage_classification/\"\n\n# Dictionary stores 12 types of trash\ncategories = {0: 'battery', 1: 'biological', 2: 'brown-glass', 3: 'cardboard', 4: 'clothes', \n              5: 'green-glass',6: 'metal', 7: 'paper', 8: 'plastic', 9: 'shoes', 10: 'trash',\n              11: 'white-glass'}\n\nprint('Defining constants successful!')","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:48.917870Z","iopub.execute_input":"2025-05-21T11:10:48.918252Z","iopub.status.idle":"2025-05-21T11:10:48.923709Z","shell.execute_reply.started":"2025-05-21T11:10:48.918234Z","shell.execute_reply":"2025-05-21T11:10:48.922773Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4: Initial data exploration - examine directories and data structures","metadata":{}},{"cell_type":"code","source":"\nprint(\"List of folders in dataset\")\nfor category in categories.values():\n    files_count = len(os.listdir(base_path + category))\n    print(f\"{category}: {files_count} image\")\n\n# Display info on original image size\ndef display_image_stats():\n    sample_categories = random.sample(list(categories.values()), 3)\n    img_paths = []\n    \n    for category in sample_categories:\n        if os.listdir(base_path + category):\n            img_paths.append(base_path + category + '/' + os.listdir(base_path + category)[0])\n    \n    print(\"\\Information on sample image size:\")\n    for path in img_paths:\n        img = Image.open(path)\n        print(f\"- Image {os.path.basename(path)} has size of: {img.size}\")\n        \n    # Check few sample to see if the image is error\n    print(\"\\nChecking for image error...\")\n    corrupted_images = []\n    \n    for category in random.sample(list(categories.values()), 3):\n        folder_path = base_path + category\n        sample_files = random.sample(os.listdir(folder_path), min(5, len(os.listdir(folder_path))))\n        \n        for filename in sample_files:\n            try:\n                img = Image.open(os.path.join(folder_path, filename))\n                img.verify()  # Xác minh ảnh không bị hỏng\n            except Exception as e:\n                corrupted_images.append((os.path.join(folder_path, filename), str(e)))\n    \n    if corrupted_images:\n        print(f\"Found {len(corrupted_images)} error image:\")\n        for img, error in corrupted_images[:5]:\n            print(f\"- {img}: {error}\")\n    else:\n        print(\"Does not found image with errors\")\n\n# Execute function to check the image\ndisplay_image_stats()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:48.924541Z","iopub.execute_input":"2025-05-21T11:10:48.924809Z","iopub.status.idle":"2025-05-21T11:10:49.091589Z","shell.execute_reply.started":"2025-05-21T11:10:48.924783Z","shell.execute_reply":"2025-05-21T11:10:49.090973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5: Display (some) sample images from each class","metadata":{}},{"cell_type":"code","source":"def display_sample_images():\n    plt.figure(figsize=(15, 12))\n    for i, category in enumerate(categories.values()):\n        folder_path = base_path + category\n        sample_images = random.sample(os.listdir(folder_path), min(3, len(os.listdir(folder_path))))\n        \n        for j, img_name in enumerate(sample_images):\n            plt.subplot(4, 9, i*3 + j + 1)\n            img = Image.open(os.path.join(folder_path, img_name))\n            plt.imshow(img)\n            plt.title(category)\n            plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Sample image display\ndisplay_sample_images()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:49.092214Z","iopub.execute_input":"2025-05-21T11:10:49.092412Z","iopub.status.idle":"2025-05-21T11:10:51.649951Z","shell.execute_reply.started":"2025-05-21T11:10:49.092380Z","shell.execute_reply":"2025-05-21T11:10:51.648985Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6: Create DataFrame contains info about image and label","metadata":{}},{"cell_type":"code","source":"# Function to add class name prefix to filename \n# Eg: \"/paper104.jpg\" thành \"paper/paper104.jpg\"\ndef add_class_name_prefix(df, col_name):\n    df[col_name] = df[col_name].apply(lambda x: x[:re.search(\"\\d\",x).start()] + '/' + x)\n    return df\n\n# List containing all file names in the dataset\nfilenames_list = []\n# Corresponding label archive list\ncategories_list = []\n\n# Go through each layer and collect all the file names and labels.\nfor category in categories:\n    filenames = os.listdir(base_path + categories[category])\n    filenames_list = filenames_list + filenames\n    categories_list = categories_list + [category] * len(filenames)\n\n# Create DataFrame from list of filenames and labels\ndf = pd.DataFrame({\n    'filename': filenames_list,\n    'category': categories_list\n})\n\n# Add class name prefix to file name\ndf = add_class_name_prefix(df, 'filename')\n\n# Randomize DataFrame\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint('Number of elements = ', len(df))\nprint('\\nFirst 5 samples:')\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:51.650740Z","iopub.execute_input":"2025-05-21T11:10:51.650955Z","iopub.status.idle":"2025-05-21T11:10:51.704332Z","shell.execute_reply.started":"2025-05-21T11:10:51.650937Z","shell.execute_reply":"2025-05-21T11:10:51.703806Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7: Display a random image from the dataset","metadata":{}},{"cell_type":"code","source":"def show_random_image():\n    random_row = random.randint(0, len(df)-1)\n    sample = df.iloc[random_row]\n    randomimage = image.load_img(base_path + sample['filename'])\n    \n    print(f\"Display images: {sample['filename']}\")\n    print(f\"Class: {categories[sample['category']]}\")\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(randomimage)\n    plt.axis('off')\n    plt.title(f\"Class: {categories[sample['category']]}\")\n    plt.show()\n\n# Display random image\nshow_random_image()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:51.704978Z","iopub.execute_input":"2025-05-21T11:10:51.705234Z","iopub.status.idle":"2025-05-21T11:10:51.866575Z","shell.execute_reply.started":"2025-05-21T11:10:51.705209Z","shell.execute_reply":"2025-05-21T11:10:51.865934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8: Analyze and visualize class distribution","metadata":{}},{"cell_type":"code","source":"def visualize_class_distribution():\n    df_visualization = df.copy()\n    # Convert labels from numbers to names\n    df_visualization['category'] = df_visualization['category'].apply(lambda x: categories[x])\n    \n    # Count the number of images in each class\n    class_counts = df_visualization['category'].value_counts()\n    \n    # Plot the class distribution chart\n    plt.figure(figsize=(14, 6))\n    ax = sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')\n    plt.xlabel(\"Types of Waste\", fontsize=12, labelpad=14)\n    plt.ylabel(\"Number of Images\", fontsize=12, labelpad=14)\n    plt.title(\"Image Count Distribution by Waste Category\", fontsize=14, y=1.02)\n    plt.xticks(rotation=45)\n    \n    # Add value labels on top of each bar\n    for p, label in zip(ax.patches, class_counts.values):\n        ax.annotate(format(label, '.0f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha='center', va='center', \n                   xytext=(0, 9), \n                   textcoords='offset points')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Check class imbalance level\n    imbalance_ratio = class_counts.max() / class_counts.min()\n    print(f\"Imbalance ratio (max/min): {imbalance_ratio:.2f}\")\n    \n    if imbalance_ratio > 1.5:\n        print(\"The dataset has significant class imbalance.\")\n        print(\"Consider using imbalance handling techniques such as class weights or data augmentation.\")\n    else:\n        print(\"The dataset is relatively balanced across classes.\")\n\n# Execute class distribution analysis\nvisualize_class_distribution()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:51.868957Z","iopub.execute_input":"2025-05-21T11:10:51.869166Z","iopub.status.idle":"2025-05-21T11:10:52.133803Z","shell.execute_reply.started":"2025-05-21T11:10:51.869150Z","shell.execute_reply":"2025-05-21T11:10:52.133152Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9: Analyze image size and other attributes","metadata":{}},{"cell_type":"code","source":"def analyze_image_properties():\n    # Select random samples for analysis\n    sample_size = 100\n    sample_files = random.sample(filenames_list, min(sample_size, len(filenames_list)))\n    \n    # Initialize lists to store information\n    widths = []\n    heights = []\n    aspect_ratios = []\n    file_sizes = []\n    \n    print(\"Analyzing image properties...\")\n    \n    for file in sample_files:\n        category = file.split('/')[0] if '/' in file else categories[df[df['filename'].str.contains(file)]['category'].iloc[0]]\n        img_path = base_path + category + '/' + os.path.basename(file)\n        \n        try:\n            img = Image.open(img_path)\n            width, height = img.size\n            widths.append(width)\n            heights.append(height)\n            aspect_ratios.append(width / height)\n            file_sizes.append(os.path.getsize(img_path) / 1024)  # Convert to KB\n        except Exception as e:\n            print(f\"Error processing image {img_path}: {e}\")\n    \n    # Display statistics\n    print(\"\\nImage dimension statistics:\")\n    print(f\"Width: Min={min(widths)}, Max={max(widths)}, Average={sum(widths)/len(widths):.2f}\")\n    print(f\"Height: Min={min(heights)}, Max={max(heights)}, Average={sum(heights)/len(heights):.2f}\")\n    print(f\"Aspect Ratio: Min={min(aspect_ratios):.2f}, Max={max(aspect_ratios):.2f}, Average={sum(aspect_ratios)/len(aspect_ratios):.2f}\")\n    print(f\"File Size: Min={min(file_sizes):.2f}KB, Max={max(file_sizes):.2f}KB, Average={sum(file_sizes)/len(file_sizes):.2f}KB\")\n    \n    # Visualize dimension distributions\n    plt.figure(figsize=(15, 10))\n    \n    plt.subplot(2, 2, 1)\n    plt.hist(widths, bins=20, color='skyblue', edgecolor='black')\n    plt.title('Width Distribution (pixels)')\n    plt.xlabel('Width')\n    plt.ylabel('Number of Images')\n    \n    plt.subplot(2, 2, 2)\n    plt.hist(heights, bins=20, color='salmon', edgecolor='black')\n    plt.title('Height Distribution (pixels)')\n    plt.xlabel('Height')\n    plt.ylabel('Number of Images')\n    \n    plt.subplot(2, 2, 3)\n    plt.hist(aspect_ratios, bins=20, color='lightgreen', edgecolor='black')\n    plt.title('Aspect Ratio Distribution (width/height)')\n    plt.xlabel('Aspect Ratio')\n    plt.ylabel('Number of Images')\n    \n    plt.subplot(2, 2, 4)\n    plt.hist(file_sizes, bins=20, color='purple', alpha=0.7, edgecolor='black')\n    plt.title('File Size Distribution (KB)')\n    plt.xlabel('Size (KB)')\n    plt.ylabel('Number of Images')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Execute image property analysis\nanalyze_image_properties()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:52.134514Z","iopub.execute_input":"2025-05-21T11:10:52.134753Z","iopub.status.idle":"2025-05-21T11:10:53.944682Z","shell.execute_reply.started":"2025-05-21T11:10:52.134731Z","shell.execute_reply":"2025-05-21T11:10:53.943949Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10: Split the data into training, validation, and test sets","metadata":{}},{"cell_type":"code","source":"# Convert labels from numbers to names before splitting the data\ndf[\"category_name\"] = df[\"category\"].apply(lambda x: categories[x])\n\n# Split the data: 70% training, 15% validation, 15% testing\ntrain_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['category'])\nvalidate_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['category'])\n\n# Reset indices\ntrain_df = train_df.reset_index(drop=True)\nvalidate_df = validate_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\n# Save dataset sizes for later use\ntotal_train = train_df.shape[0]\ntotal_validate = validate_df.shape[0]\ntotal_test = test_df.shape[0]\n\nprint('Training set size =', total_train)\nprint('Validation set size =', total_validate)\nprint('Test set size =', total_test)\n\n# Check class distribution in each dataset\nprint(\"\\nClass distribution in training set:\")\nprint(train_df['category_name'].value_counts())\n\nprint(\"\\nClass distribution in validation set:\")\nprint(validate_df['category_name'].value_counts())\n\nprint(\"\\nClass distribution in test set:\")\nprint(test_df['category_name'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:53.945480Z","iopub.execute_input":"2025-05-21T11:10:53.945775Z","iopub.status.idle":"2025-05-21T11:10:53.973127Z","shell.execute_reply.started":"2025-05-21T11:10:53.945759Z","shell.execute_reply":"2025-05-21T11:10:53.972380Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11: Calculate class weights to handle data imbalance","metadata":{}},{"cell_type":"code","source":"def calculate_class_weights():\n    # Calculate the number of samples in each class\n    class_counts = train_df['category'].value_counts().sort_index()\n    \n    # Compute class weights\n    n_samples = len(train_df)\n    n_classes = len(categories)\n    \n    class_weights = {}\n    for i, count in enumerate(class_counts):\n        class_weights[i] = n_samples / (n_classes * count)\n    \n    # Normalize class weights\n    max_weight = max(class_weights.values())\n    for i in class_weights:\n        class_weights[i] = class_weights[i] / max_weight\n    \n    print(\"Calculated class weights:\")\n    for i, weight in class_weights.items():\n        print(f\"{categories[i]}: {weight:.4f}\")\n    \n    return class_weights\n\n# Compute class weights\nclass_weights = calculate_class_weights()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:53.973913Z","iopub.execute_input":"2025-05-21T11:10:53.974685Z","iopub.status.idle":"2025-05-21T11:10:53.983720Z","shell.execute_reply.started":"2025-05-21T11:10:53.974660Z","shell.execute_reply":"2025-05-21T11:10:53.983122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 12: Prepare data generators with data augmentation","metadata":{}},{"cell_type":"code","source":"def create_data_generators():\n    batch_size = 32\n    \n    # Generator for the training set with data augmentation\n    train_datagen = image.ImageDataGenerator(\n        rotation_range=20,             # Randomly rotate images within 20 degrees\n        width_shift_range=0.1,         # Horizontally shift images by 10%\n        height_shift_range=0.1,        # Vertically shift images by 10%\n        shear_range=0.1,               # Apply shear transformations\n        zoom_range=0.2,                # Zoom in/out by 20%\n        horizontal_flip=True,          # Flip images horizontally\n        fill_mode='nearest'            # Fill in missing pixels\n    )\n    \n    # Generator for validation and test sets (no augmentation)\n    valid_test_datagen = image.ImageDataGenerator()\n    \n    # Create generator for the training set\n    train_generator = train_datagen.flow_from_dataframe(\n        train_df,\n        base_path,\n        x_col='filename',\n        y_col='category_name',\n        target_size=IMAGE_SIZE,\n        class_mode='categorical',\n        batch_size=batch_size,\n        shuffle=True\n    )\n    \n    # Create generator for the validation set\n    validation_generator = valid_test_datagen.flow_from_dataframe(\n        validate_df,\n        base_path,\n        x_col='filename',\n        y_col='category_name',\n        target_size=IMAGE_SIZE,\n        class_mode='categorical',\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    # Create generator for the test set\n    test_generator = valid_test_datagen.flow_from_dataframe(\n        test_df,\n        base_path,\n        x_col='filename',\n        y_col='category_name',\n        target_size=IMAGE_SIZE,\n        class_mode='categorical',\n        batch_size=1,  # Batch size 1 to make evaluating individual samples easier\n        shuffle=False\n    )\n    \n    # Save the mapping dictionary between class names and indices\n    class_indices = train_generator.class_indices\n    class_indices_inv = {v: k for k, v in class_indices.items()}\n    \n    print(\"Class indices:\")\n    for cls_name, idx in class_indices.items():\n        print(f\"{cls_name}: {idx}\")\n    \n    return train_generator, validation_generator, test_generator, class_indices, class_indices_inv, batch_size\n\n# Create generators\ntrain_generator, validation_generator, test_generator, class_indices, class_indices_inv, batch_size = create_data_generators()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:53.984457Z","iopub.execute_input":"2025-05-21T11:10:53.984652Z","iopub.status.idle":"2025-05-21T11:11:01.750346Z","shell.execute_reply.started":"2025-05-21T11:10:53.984637Z","shell.execute_reply":"2025-05-21T11:11:01.749796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 13: Show some images with data augmentation applied","metadata":{}},{"cell_type":"code","source":"def show_augmented_images():\n    # Select a random image sample from the training set\n    sample_df = train_df.sample(1)\n    sample_class = sample_df['category_name'].values[0]\n    sample_path = base_path + sample_df['filename'].values[0]\n    \n    # Load the image and prepare it for augmentation\n    sample_img = image.load_img(sample_path, target_size=IMAGE_SIZE)\n    x = image.img_to_array(sample_img)\n    x = x.reshape((1,) + x.shape)\n    \n    # Create a temporary generator with the same augmentation settings as train_datagen\n    aug_datagen = image.ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        shear_range=0.1,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    \n    # Plot the original and augmented images\n    plt.figure(figsize=(15, 8))\n    \n    # Show the original image\n    plt.subplot(2, 4, 1)\n    plt.imshow(sample_img)\n    plt.title('Original Image')\n    plt.axis('off')\n    \n    # Show the augmented images\n    i = 1\n    for batch in aug_datagen.flow(x, batch_size=1):\n        plt.subplot(2, 4, i+1)\n        plt.imshow(image.array_to_img(batch[0]))\n        plt.title(f'Augmented #{i}')\n        plt.axis('off')\n        i += 1\n        if i > 7:  # Show up to 7 augmented images\n            break\n    \n    plt.suptitle(f'Augmented Images - Class: {sample_class}', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n# Show augmented images\nshow_augmented_images()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:11:01.750990Z","iopub.execute_input":"2025-05-21T11:11:01.751227Z","iopub.status.idle":"2025-05-21T11:11:02.640661Z","shell.execute_reply.started":"2025-05-21T11:11:01.751210Z","shell.execute_reply":"2025-05-21T11:11:02.639872Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 14: Modeling with transfer learning using Xception","metadata":{}},{"cell_type":"code","source":"def create_xception_model():\n    # Load pretrained Xception model without the final classification layer\n    xception_layer = xception.Xception(\n        include_top=False, \n        input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),\n        weights='../input/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    )\n    \n    # Freeze layers in Xception (do not retrain)\n    xception_layer.trainable = False\n    \n    # Create sequential model\n    model = Sequential([\n        # Input layer\n        keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)),\n        \n        # Preprocessing function for Xception\n        Lambda(lambda img: xception.preprocess_input(img)),\n        \n        # Add Xception base model\n        xception_layer,\n        \n        # Add new classification layers\n        GlobalAveragePooling2D(),\n        Dense(512, activation='relu'),\n        Dropout(0.3),  # Add dropout to reduce overfitting\n        Dense(256, activation='relu'),\n        Dropout(0.2),\n        Dense(len(categories), activation='softmax')\n    ])\n    \n    # Compile the model\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['categorical_accuracy']\n    )\n    \n    return model\n\n# Create model\nmodel = create_xception_model()\n\n# Show model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:11:02.641509Z","iopub.execute_input":"2025-05-21T11:11:02.641769Z","iopub.status.idle":"2025-05-21T11:11:06.889849Z","shell.execute_reply.started":"2025-05-21T11:11:02.641749Z","shell.execute_reply":"2025-05-21T11:11:06.889168Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 15: Prepare callbacks to monitor and adjust the training process","metadata":{}},{"cell_type":"code","source":"def create_callbacks():\n    # Early stopping to stop training when performance doesn't improve\n    early_stop = EarlyStopping(\n        monitor='val_categorical_accuracy',\n        patience=5,\n        verbose=1,\n        mode='max',\n        min_delta=0.001,\n        restore_best_weights=True\n    )\n    \n    # Reduce learning rate when performance plateaus\n    reduce_lr = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        verbose=1,\n        min_delta=0.001,\n        min_lr=1e-6\n    )\n    \n    # Save the best model - use .keras format instead of .h5\n    checkpoint = ModelCheckpoint(\n        'best_garbage_classification_model.keras',\n        monitor='val_categorical_accuracy',\n        mode='max',\n        verbose=1,\n        save_best_only=True\n    )\n    \n    return [early_stop, reduce_lr, checkpoint]\n\n# Create callbacks\ncallbacks = create_callbacks()","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:11:06.890650Z","iopub.execute_input":"2025-05-21T11:11:06.890844Z","iopub.status.idle":"2025-05-21T11:11:06.895715Z","shell.execute_reply.started":"2025-05-21T11:11:06.890820Z","shell.execute_reply":"2025-05-21T11:11:06.895047Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 16: Model training","metadata":{}},{"cell_type":"code","source":"def train_model(epochs=20):\n    print(\"Starting model training...\")\n\n    # Record start time\n    start_time = time.time()\n\n    # Train the model\n    history = model.fit(\n        train_generator,\n        epochs=epochs,\n        validation_data=validation_generator,\n        callbacks=callbacks,\n        class_weight=class_weights,  # Use class weights to handle imbalance\n        verbose=1\n    )\n\n    # Calculate training time\n    training_time = time.time() - start_time\n    hours = int(training_time // 3600)\n    minutes = int((training_time % 3600) // 60)\n    seconds = int(training_time % 60)\n\n    print(f\"Training completed in {hours}h {minutes}m {seconds}s\")\n\n    return history\n\n# Train the model\nhistory = train_model(epochs=20)\n\n# Save model weights - use .keras format instead of .h5\nmodel.save(\"/kaggle/working/garbage_classification_model.keras\")\nprint(\"Model saved successfully!\")","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:11:06.896517Z","iopub.execute_input":"2025-05-21T11:11:06.896918Z","iopub.status.idle":"2025-05-21T12:37:39.584837Z","shell.execute_reply.started":"2025-05-21T11:11:06.896896Z","shell.execute_reply":"2025-05-21T12:37:39.584167Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is a difference of about 2.2% between:\n\nTraining accuracy: 98.03%\nValidation accuracy: 95.83%\n\nThis difference suggests a slight degree of overfitting — the model performs better on the seen data (training set) compared to the unseen data (validation set). However, the gap is not too large, so it is still acceptable.","metadata":{}},{"cell_type":"markdown","source":"# 17: Visualize the training process","metadata":{}},{"cell_type":"code","source":"def visualize_training_history(history):\n    # Create figure with 2 subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n    \n    # Plot loss graph\n    ax1.plot(history.history['loss'], color='blue', linestyle='-', linewidth=2, label=\"Training loss\")\n    ax1.plot(history.history['val_loss'], color='red', linestyle='-', linewidth=2, label=\"Validation loss\")\n    ax1.set_title('Model Loss', fontsize=14)\n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.grid(True, linestyle='--', alpha=0.6)\n    ax1.legend(fontsize=10)\n    \n    # Plot accuracy graph\n    ax2.plot(history.history['categorical_accuracy'], color='blue', linestyle='-', linewidth=2, label=\"Training accuracy\")\n    ax2.plot(history.history['val_categorical_accuracy'], color='red', linestyle='-', linewidth=2, label=\"Validation accuracy\")\n    ax2.set_title('Model Accuracy', fontsize=14)\n    ax2.set_xlabel('Epoch', fontsize=12)\n    ax2.set_ylabel('Accuracy', fontsize=12)\n    ax2.grid(True, linestyle='--', alpha=0.6)\n    ax2.legend(fontsize=10)\n    \n    # Find epoch with highest validation accuracy\n    best_epoch = np.argmax(history.history['val_categorical_accuracy'])\n    best_acc = history.history['val_categorical_accuracy'][best_epoch]\n    \n    # Mark best epoch\n    ax2.annotate(f'Best: {best_acc:.4f}', \n                xy=(best_epoch, best_acc),\n                xytext=(best_epoch, best_acc-0.1),\n                arrowprops=dict(facecolor='black', shrink=0.05),\n                fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Show additional info\n    print(f\"Starting training accuracy: {history.history['categorical_accuracy'][0]:.4f}\")\n    print(f\"Final training accuracy: {history.history['categorical_accuracy'][-1]:.4f}\")\n    print(f\"Starting validation accuracy: {history.history['val_categorical_accuracy'][0]:.4f}\")\n    print(f\"Final validation accuracy: {history.history['val_categorical_accuracy'][-1]:.4f}\")\n    print(f\"Best validation accuracy: {best_acc:.4f} at epoch {best_epoch+1}\")\n\n# Show training process plots\nvisualize_training_history(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:37:39.585598Z","iopub.execute_input":"2025-05-21T12:37:39.585843Z","iopub.status.idle":"2025-05-21T12:37:40.029022Z","shell.execute_reply.started":"2025-05-21T12:37:39.585826Z","shell.execute_reply":"2025-05-21T12:37:40.028259Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 18: Performance evaluation on test set","metadata":{}},{"cell_type":"code","source":"def evaluate_model():\n    print(\"Evaluating model on test set...\")\n    \n    # Evaluate overall performance\n    test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n    print(f'Accuracy on test set: {test_accuracy*100:.2f}%')\n    \n    # Get predictions\n    print(\"Generating predictions for test set...\")\n    test_generator.reset()  # Reset generator to start\n    \n    # Predict on test set\n    y_pred = model.predict(test_generator, steps=len(test_generator), verbose=1)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    \n    # Convert true labels from generators\n    y_true = test_generator.classes\n    \n    # Create classification report\n    print(\"\\nDetailed classification report:\")\n    # Use class names from class_indices_inv\n    class_names = [class_indices_inv[i] for i in range(len(class_indices_inv))]\n    print(classification_report(y_true, y_pred_classes, target_names=class_names))\n    \n    return y_true, y_pred_classes, class_names\n\n# Perform evaluation\ny_true, y_pred_classes, class_names = evaluate_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:37:40.029837Z","iopub.execute_input":"2025-05-21T12:37:40.030091Z","iopub.status.idle":"2025-05-21T12:38:33.514288Z","shell.execute_reply.started":"2025-05-21T12:37:40.030060Z","shell.execute_reply":"2025-05-21T12:38:33.513549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 19: Plot confusion matrix","metadata":{}},{"cell_type":"code","source":"def display_confusion_matrix(y_true, y_pred_classes, class_names):\n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Create normalized confusion matrix\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    class_names = [class_indices_inv[i] for i in range(len(class_indices_inv))]\n    \n    # Set figure size based on number of classes\n    plt.figure(figsize=(16, 14))\n    \n    # Display confusion matrix - without showing numbers\n    plt.figure(figsize=(16, 14))\n    \n    ax = plt.subplot()\n    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names,\n                annot_kws={\"size\": 12}, # Tăng kích thước chữ số\n                cbar_kws={\"shrink\": 0.8})\n    \n    plt.title('Normalized Confusion Matrix', fontsize=18)\n    plt.xlabel('Predicted Label', fontsize=14)\n    plt.ylabel('True Label', fontsize=14)\n    plt.xticks(fontsize=12, rotation=45, ha='right')\n    plt.yticks(fontsize=12)\n    plt.tight_layout()\n    plt.show()\n    print(\"5 most common misclassification:\")\n    misclassifications = []\n    \n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            if i != j and cm[i, j] > 0:\n                misclassifications.append((\n                    class_names[i],   # Lớp thực tế\n                    class_names[j],   # Lớp dự đoán\n                    cm[i, j],         # Số lượng\n                    cm[i, j]/np.sum(cm[i, :])*100  # Phần trăm\n                ))\n    \n    misclassifications.sort(key=lambda x: x[2], reverse=True)\n    \n    \n    for true_cls, pred_cls, count, percentage in misclassifications[:5]:\n        print(f\"- True: {true_cls}, Prediction: {pred_cls}, Count: {int(count)} ({percentage:.2f}%)\")\n\n# Display confusion matrix\ndisplay_confusion_matrix(y_true, y_pred_classes, class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:05:37.183759Z","iopub.execute_input":"2025-05-21T14:05:37.183964Z","iopub.status.idle":"2025-05-21T14:05:37.746538Z","shell.execute_reply.started":"2025-05-21T14:05:37.183948Z","shell.execute_reply":"2025-05-21T14:05:37.745759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_misclassifications():\n    # Find indices of misclassified samples\n    misclassified_indices = np.where(y_pred_classes != y_true)[0]\n    \n    if len(misclassified_indices) == 0:\n        print(\"No misclassified samples found in the test set!\")\n        return\n    \n    print(f\"Found {len(misclassified_indices)} misclassified samples.\")\n    \n    # Display some misclassified samples\n    num_examples = min(10, len(misclassified_indices))\n    fig = plt.figure(figsize=(15, num_examples * 2))\n    \n    for i, idx in enumerate(misclassified_indices[:num_examples]):\n        # Get image path\n        img_path = base_path + test_df.iloc[idx]['filename']\n        \n        # Load and display image\n        img = image.load_img(img_path, target_size=IMAGE_SIZE)\n        \n        # Process image for prediction\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = xception.preprocess_input(x)\n        \n        # Detailed prediction\n        preds = model.predict(x)\n        pred_prob = preds[0][y_pred_classes[idx]]  # Probability of predicted class\n        \n        # True and predicted class names\n        true_class = class_indices_inv[y_true[idx]]\n        pred_class = class_indices_inv[y_pred_classes[idx]]\n        \n        # Show image and info\n        plt.subplot(num_examples, 2, i*2 + 1)\n        plt.imshow(img)\n        plt.title(f\"True: {true_class}\\nPredicted: {pred_class} ({pred_prob:.2f})\")\n        plt.axis('off')\n        \n        # Show prediction probability distribution\n        plt.subplot(num_examples, 2, i*2 + 2)\n        \n        # Get top 5 classes with highest probability\n        top_indices = np.argsort(preds[0])[-5:][::-1]\n        top_classes = [class_indices_inv[idx] for idx in top_indices]\n        top_probs = [preds[0][idx] for idx in top_indices]\n        \n        plt.barh(top_classes, top_probs, color='skyblue')\n        plt.xlabel('Probability')\n        plt.title('Top 5 classes with highest probability')\n        plt.xlim(0, 1)\n        plt.grid(axis='x', linestyle='--', alpha=0.6)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Error analysis by class\n    print(\"\\nError analysis by class:\")\n    for class_idx in range(len(class_names)):\n        # Get samples belonging to this class\n        class_samples = np.where(y_true == class_idx)[0]\n        \n        if len(class_samples) == 0:\n            continue\n        \n        # Count misclassified samples\n        class_errors = np.where(y_pred_classes[class_samples] != class_idx)[0]\n        error_rate = len(class_errors) / len(class_samples) * 100\n        \n        print(f\"{class_names[class_idx]}: {error_rate:.2f}% error ({len(class_errors)}/{len(class_samples)})\")\n\n# Analyze misclassified samples\nanalyze_misclassifications()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:38:33.947891Z","iopub.execute_input":"2025-05-21T12:38:33.948170Z","iopub.status.idle":"2025-05-21T12:38:39.214882Z","shell.execute_reply.started":"2025-05-21T12:38:33.948137Z","shell.execute_reply":"2025-05-21T12:38:39.214278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 21: Fine-tuning the model to improve performance","metadata":{}},{"cell_type":"code","source":"def fine_tune_model():\n    global model\n    \n    print(\"Starting fine-tuning the model...\")\n    \n    print(\"Checking model structure before fine-tuning:\")\n    for i, layer in enumerate(model.layers):\n        print(f\"Layer {i}: {layer.name}, Trainable: {layer.trainable}\")\n    \n    xception_layer = None\n    for layer in model.layers:\n        if isinstance(layer, keras.layers.Lambda):\n            continue\n        if not isinstance(layer, keras.layers.GlobalAveragePooling2D):\n            xception_layer = layer\n            break\n    \n    if xception_layer is None:\n        print(\"Cannot find Xception layer, trying alternative method...\")\n        new_model = create_xception_model()\n        \n        xception_found = False\n        for layer in new_model.layers:\n            if hasattr(layer, 'layers') and len(getattr(layer, 'layers', [])) > 50:\n                xception_found = True\n                break\n        \n        if not xception_found:\n            print(\"Cannot find Xception layer in new model. Skipping fine-tuning.\")\n            return None\n        \n        new_model.set_weights(model.get_weights())\n        model = new_model\n    \n    print(\"\\nCreating new model with last layers unfrozen...\")\n    \n    base_model = xception.Xception(\n        include_top=False,\n        input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),\n        weights='../input/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    )\n    \n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    for layer in base_model.layers[-20:]:\n        layer.trainable = True\n    \n    trainable_count = sum(1 for layer in base_model.layers if layer.trainable)\n    non_trainable_count = sum(1 for layer in base_model.layers if not layer.trainable)\n    \n    print(f\"Trainable layers in Xception: {trainable_count}\")\n    print(f\"Non-trainable layers in Xception: {non_trainable_count}\")\n    \n    inputs = keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS))\n    x = Lambda(lambda img: xception.preprocess_input(img))(inputs)\n    \n    x = base_model(x)\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    outputs = Dense(len(categories), activation='softmax')(x)\n    \n    new_model = Model(inputs, outputs)\n    \n    new_model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n        loss='categorical_crossentropy',\n        metrics=['categorical_accuracy']\n    )\n    \n    fine_tune_callbacks = [\n        EarlyStopping(\n            monitor='val_categorical_accuracy',\n            patience=5,\n            verbose=1,\n            mode='max',\n            restore_best_weights=True\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            verbose=1,\n            min_lr=1e-7\n        ),\n        ModelCheckpoint(\n            'fine_tuned_model.keras',\n            monitor='val_categorical_accuracy',\n            mode='max',\n            verbose=1,\n            save_best_only=True\n        )\n    ]\n    \n    print(\"\\nTraining model with fine-tuning:\")\n    fine_tune_history = new_model.fit(\n        train_generator,\n        epochs=10,\n        validation_data=validation_generator,\n        callbacks=fine_tune_callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    new_model.save(\"/kaggle/working/fine_tuned_garbage_model.keras\")\n    print(\"Fine-tuned model saved successfully!\")\n    \n    model = new_model\n    \n    return fine_tune_history\n\nfine_tune_history = fine_tune_model()\n\nif fine_tune_history is not None:\n    visualize_training_history(fine_tune_history)\nelse:\n    print(\"Could not perform fine-tuning. Continuing with the current model.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:38:39.215849Z","iopub.execute_input":"2025-05-21T12:38:39.216414Z","iopub.status.idle":"2025-05-21T13:21:58.554804Z","shell.execute_reply.started":"2025-05-21T12:38:39.216367Z","shell.execute_reply":"2025-05-21T13:21:58.554091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This result shows the structure of your current model before fine-tuning:\n\nLayer 0: lambda, Trainable: True\n\nThis is the first Lambda layer, responsible for preprocessing the input data.\nThis layer applies the preprocessing function xception.preprocess_input to prepare the images for the Xception model.\nAlthough marked as trainable (Trainable: True), it actually has no parameters to train.\n\nLayer 1: xception, Trainable: False\n\nThis is the Xception model used as the backbone for transfer learning.\nThis layer is set as non-trainable (Trainable: False), meaning during initial training you froze the Xception parameters.\nBy freezing Xception, you use the knowledge it learned from a large dataset (like ImageNet) without changing its parameters.\n\nLayer 2: global_average_pooling2d, Trainable: True\n\nThis is the GlobalAveragePooling2D layer, which reduces the output tensor size from Xception.\nThis layer has no parameters to train but is still marked as Trainable: True.\n\nLayer 3: dense, Trainable: True\n\nThis is the first Dense layer with 512 neurons, using ReLU activation.\nIt is trainable and contains many parameters (weights and biases).\n\nLayer 4: dropout, Trainable: True\n\nThis is a Dropout layer with a rate of 0.3, helping to reduce overfitting.\nIt has no trainable parameters.\n\nLayer 5: dense_1, Trainable: True\n\nThis is the second Dense layer with 256 neurons, also with ReLU activation.\nIt is trainable.\n\nLayer 6: dropout_1, Trainable: True\n\nAnother Dropout layer with a rate of 0.2.\nNo trainable parameters.\n\nLayer 7: dense_2, Trainable: True\n\nThis is the final Dense layer with the number of neurons equal to the number of classes (12 classes) and softmax activation.\nIt performs the final classification and is trainable.\n\nMeaning for fine-tuning:\n\nCurrently, you have a model with the Xception backbone frozen (Trainable: False) and the custom classification layers trained (Trainable: True).\nIn fine-tuning, the goal is to unlock some of the last layers of Xception to allow them to adapt to your specific data.\nHowever, you cannot directly access the inner layers of Xception from this Sequential model (which caused the original error).\nTherefore, the approach in cell 21 is to create a new model with a similar structure but unlock the last layers of Xception.\n\nCell 21 was adjusted to handle this by:\n\n    Creating a new Xception model\n    Freezing most layers except the last 20 layers\n    Building a new model with a similar structure\n    Performing fine-tuning with a lower learning rate\n\nThis process allows the model to fine-tune higher-level features in Xception to better fit your garbage classification data, while keeping the low-level features learned from ImageNet intact.","metadata":{}},{"cell_type":"markdown","source":"Comments and Explanation\n\nTraining Accuracy:\n\nThe Transfer Learning model has higher accuracy (98.03% vs. 93.71%).\nThis may be because the Fine-tuning model is more complex (more parameters are trained), so it requires more epochs to reach the highest accuracy.\n\nValidation Accuracy:\n\nBoth models achieve high accuracy on the validation set (>94.9%).\nThe Transfer Learning model has slightly higher accuracy (95.83% vs. 94.97%).\nThis difference is not significant and could be due to random noise.\n\nGeneralization Ability:\n\nAlthough the Transfer Learning model has higher training accuracy, the gap between training and validation accuracy is quite large (98.03% - 95.83% = 2.2%).\nThis gap is smaller in the Fine-tuning model (93.71% - 94.97% = -1.26%), meaning the model actually performs better on unseen data.\nThis suggests the Fine-tuning model generalizes better.\n\nOverfitting Issue:\n\nThe Transfer Learning model shows mild overfitting (training accuracy significantly higher than validation accuracy).\nThe Fine-tuning model shows no signs of overfitting (validation accuracy is even higher than training accuracy).\n\nWhich Model is Better?\n\nThe Fine-tuning model (cell 21) can be considered better in terms of generalization for the following reasons:\n\n    Better generalization: The Fine-tuning model has a smaller gap between training and validation accuracy, even performing better on unseen data.\n\n    Less overfitting: The Fine-tuning model shows no overfitting, while the Transfer Learning model has slight overfitting.\n\n    Potential for improvement: The Fine-tuning model could achieve better performance with more training epochs, as its training curve is still improving.\n\n    More specific features: By unlocking the last layers of Xception, the Fine-tuning model can learn features more specific to garbage classification rather than relying solely on general features learned from ImageNet.\n\nHowever, if the goal is the highest accuracy on the validation set without much concern for overfitting, the original Transfer Learning model remains a good choice with a validation accuracy of 95.83%.\n\nConclusion: For practical deployment, I recommend using the Fine-tuning model (cell 21) because of its better generalization ability and potential to perform better on real unseen data.","metadata":{}},{"cell_type":"code","source":"# CELL 21B: Continue fine-tuning the model with additional epochs\ndef continue_fine_tuning(additional_epochs=10):\n    print(\"Continuing fine-tuning the model with additional epochs...\")\n    \n    continue_callbacks = [\n        EarlyStopping(\n            monitor='val_categorical_accuracy',\n            patience=5,\n            verbose=1,\n            mode='max',\n            restore_best_weights=True\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            verbose=1,\n            min_lr=1e-7\n        ),\n        ModelCheckpoint(\n            'continued_fine_tuned_model.keras',\n            monitor='val_categorical_accuracy',\n            mode='max',\n            verbose=1,\n            save_best_only=True\n        )\n    ]\n    \n    start_time = time.time()\n    \n    print(f\"\\nContinuing training for {additional_epochs} epochs:\")\n    continued_history = model.fit(\n        train_generator,\n        epochs=additional_epochs,\n        validation_data=validation_generator,\n        callbacks=continue_callbacks,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    training_time = time.time() - start_time\n    hours = int(training_time // 3600)\n    minutes = int((training_time % 3600) // 60)\n    seconds = int(training_time % 60)\n    \n    print(f\"Completed additional fine-tuning in {hours}h {minutes}m {seconds}s\")\n    \n    model.save(\"/kaggle/working/continued_fine_tuned_model.keras\")\n    print(\"Continued fine-tuned model saved successfully!\")\n    \n    return continued_history\n\ncontinued_fine_tune_history = continue_fine_tuning(additional_epochs=10)\n\nif continued_fine_tune_history is not None:\n    visualize_training_history(continued_fine_tune_history)\n    \n    if 'fine_tune_history' in globals() and fine_tune_history is not None:\n        try:\n            combined_acc = fine_tune_history.history['categorical_accuracy'] + continued_fine_tune_history.history['categorical_accuracy']\n            combined_val_acc = fine_tune_history.history['val_categorical_accuracy'] + continued_fine_tune_history.history['val_categorical_accuracy']\n            combined_loss = fine_tune_history.history['loss'] + continued_fine_tune_history.history['loss']\n            combined_val_loss = fine_tune_history.history['val_loss'] + continued_fine_tune_history.history['val_loss']\n            \n            class CombinedHistory:\n                def __init__(self, acc, val_acc, loss, val_loss):\n                    self.history = {\n                        'categorical_accuracy': acc,\n                        'val_categorical_accuracy': val_acc,\n                        'loss': loss,\n                        'val_loss': val_loss\n                    }\n            \n            combined_history = CombinedHistory(combined_acc, combined_val_acc, combined_loss, combined_val_loss)\n            \n            print(\"\\nCombined training progress chart of both fine-tuning phases:\")\n            visualize_training_history(combined_history)\n            \n            best_epoch = np.argmax(combined_val_acc)\n            best_acc = combined_val_acc[best_epoch]\n            \n            print(f\"Best validation accuracy: {best_acc:.4f} at epoch {best_epoch+1} (combined process)\")\n            \n        except Exception as e:\n            print(f\"Could not combine training histories: {e}\")\nelse:\n    print(\"Cannot continue fine-tuning. Please check the model status.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:21:58.555731Z","iopub.execute_input":"2025-05-21T13:21:58.556017Z","iopub.status.idle":"2025-05-21T14:04:42.681273Z","shell.execute_reply.started":"2025-05-21T13:21:58.555992Z","shell.execute_reply":"2025-05-21T14:04:42.680604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The chart after fine-tuning shows many improvements compared to the original model:\n\n    Higher performance: The best accuracy increased from 95.83% to 96.78%\n    Better generalization: The gap between training and validation accuracy is very small, indicating better generalization ability\n    More stable: The validation accuracy curve rises steadily and smoothly, with little fluctuation\n    No overfitting: There is no training accuracy significantly surpassing validation accuracy as seen in the original model","metadata":{}},{"cell_type":"markdown","source":"The accuracy is already very high: With a validation accuracy of 96.78%, your model has achieved excellent performance for the garbage classification task. This is an impressive result for a real-world problem.\nConvergence has been reached: The model achieved its best accuracy at epoch 18 and has likely converged. Training for more epochs may not bring significant improvement.\nRisk of overfitting: If training continues, there is a risk the model will start overfitting the training data, causing performance to drop on new data.\nGap between training and validation: The gap between training accuracy (96.90%) and validation accuracy (96.78%) is very small (only 0.12%), indicating good generalization ability and no overfitting.\nStable performance: The final accuracy (96.65%) is close to the best accuracy (96.78%), showing the model is quite stable.","metadata":{}},{"cell_type":"markdown","source":"# 22: Model evaluation after fine-tuning","metadata":{}},{"cell_type":"code","source":"def evaluate_fine_tuned_model():\n    print(\"Evaluating fine-tuned model on the test set...\")\n    \n    # Evaluate overall performance\n    test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n    print(f'Accuracy on test set (after fine-tuning): {test_accuracy*100:.2f}%')\n    \n    # Get predictions\n    print(\"Generating predictions for the test set...\")\n    test_generator.reset()  # Reset generator to start\n    \n    # Predict on the test set\n    y_pred_ft = model.predict(test_generator, steps=len(test_generator), verbose=1)\n    y_pred_ft_classes = np.argmax(y_pred_ft, axis=1)\n    \n    # Convert true labels from generator\n    y_true = test_generator.classes\n    \n    # Create detailed classification report\n    print(\"\\nDetailed classification report after fine-tuning:\")\n    # Use class names from class_indices_inv\n    class_names = [class_indices_inv[i] for i in range(len(class_indices_inv))]\n    ft_report = classification_report(y_true, y_pred_ft_classes, target_names=class_names, output_dict=True)\n    print(classification_report(y_true, y_pred_ft_classes, target_names=class_names))\n    \n    # Analyze improvements per class (if data from model before fine-tuning exists)\n    if 'y_pred_classes' in globals():\n        print(\"\\nComparing performance before and after fine-tuning:\")\n        \n        # Calculate accuracy before and after fine-tuning\n        pre_ft_accuracy = accuracy_score(y_true, y_pred_classes)\n        post_ft_accuracy = accuracy_score(y_true, y_pred_ft_classes)\n        \n        print(f\"Overall accuracy - Before: {pre_ft_accuracy*100:.2f}%, After: {post_ft_accuracy*100:.2f}%\")\n        print(f\"Overall improvement: {(post_ft_accuracy - pre_ft_accuracy)*100:.2f}%\")\n        \n        # Compare per class\n        pre_report = classification_report(y_true, y_pred_classes, target_names=class_names, output_dict=True)\n        \n        print(\"\\nF1-score improvements per class:\")\n        for cls in class_names:\n            pre_f1 = pre_report[cls]['f1-score']\n            post_f1 = ft_report[cls]['f1-score']\n            change = post_f1 - pre_f1\n            change_percent = (change / pre_f1) * 100 if pre_f1 > 0 else float('inf')\n            \n            direction = \"+\" if change >= 0 else \"\"\n            print(f\"{cls:12}: {pre_f1:.3f} -> {post_f1:.3f} ({direction}{change:.3f}, {direction}{change_percent:.1f}%)\")\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred_ft_classes)\n    \n    # Find the most common misclassifications\n    print(\"\\nTop most common misclassifications:\")\n    error_counts = []\n    \n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            if i != j and cm[i, j] > 0:\n                error_counts.append((class_names[i], class_names[j], cm[i, j], cm[i, j]/np.sum(cm[i, :])))\n    \n    # Sort by error count descending\n    error_counts.sort(key=lambda x: x[2], reverse=True)\n    \n    # Show top 10 errors\n    for true_cls, pred_cls, count, rate in error_counts[:10]:\n        print(f\"- True: {true_cls}, Predicted: {pred_cls}, Count: {count} ({rate*100:.1f}% of class {true_cls})\")\n    \n    return y_true, y_pred_ft_classes, class_names, ft_report\n\n# Run evaluation of fine-tuned model\ny_true_ft, y_pred_ft_classes, class_names_ft, ft_report = evaluate_fine_tuned_model()\n\n# Plot confusion matrix for fine-tuned model\ndef plot_fine_tuned_confusion_matrix():\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true_ft, y_pred_ft_classes)\n    \n    # Create normalized confusion matrix\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    class_names = [class_indices_inv[i] for i in range(len(class_indices_inv))]\n    \n    # Set figure size based on number of classes\n    plt.figure(figsize=(16, 14))\n\n    # Display confusion matrix    \n    ax = plt.subplot()\n    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names,\n                annot_kws={\"size\": 12}, # Tăng kích thước chữ số\n                cbar_kws={\"shrink\": 0.8})\n        \n    plt.title('Confusion Matrix after Fine-tuning (Normalized)', fontsize=16)\n    plt.xlabel('Predicted Label', fontsize=14)\n    plt.ylabel('True Label', fontsize=14)\n    plt.xticks(fontsize=12, rotation=45, ha='right')\n    plt.yticks(fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\n# Plot confusion matrix\nplot_fine_tuned_confusion_matrix()\n\n# Visualize per-class metrics\ndef plot_class_metrics():\n    # Extract metrics\n    precisions = [ft_report[cls]['precision'] for cls in class_names_ft]\n    recalls = [ft_report[cls]['recall'] for cls in class_names_ft]\n    f1_scores = [ft_report[cls]['f1-score'] for cls in class_names_ft]\n    supports = [ft_report[cls]['support'] for cls in class_names_ft]\n    \n    # Create figure\n    plt.figure(figsize=(14, 8))\n    \n    # Create bar plot\n    x = np.arange(len(class_names_ft))\n    width = 0.25\n    \n    plt.bar(x - width, precisions, width, label='Precision', color='royalblue')\n    plt.bar(x, recalls, width, label='Recall', color='seagreen')\n    plt.bar(x + width, f1_scores, width, label='F1-score', color='darkorange')\n    \n    # Add labels and legend\n    plt.xlabel('Class', fontsize=12)\n    plt.ylabel('Score', fontsize=12)\n    plt.title('Classification Performance per Class after Fine-tuning', fontsize=14)\n    plt.xticks(x, class_names_ft, rotation=45, ha='right')\n    plt.ylim(0, 1.0)\n    plt.legend()\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    # Plot support counts\n    plt.figure(figsize=(14, 6))\n    plt.bar(class_names_ft, supports, color='slateblue')\n    plt.xlabel('Class', fontsize=12)\n    plt.ylabel('Number of Samples', fontsize=12)\n    plt.title('Sample Distribution in Test Set', fontsize=14)\n    plt.xticks(rotation=45, ha='right')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n# Plot class metrics\nplot_class_metrics()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:24:26.552669Z","iopub.execute_input":"2025-05-21T14:24:26.552969Z","iopub.status.idle":"2025-05-21T14:25:15.946489Z","shell.execute_reply.started":"2025-05-21T14:24:26.552947Z","shell.execute_reply":"2025-05-21T14:25:15.945710Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Common Misclassification Analysis\n\nWhite-glass misclassified as Plastic: 9 cases (7.8% of the white-glass class)\nThis suggests that some white glass samples share visual characteristics with plastic, possibly due to transparency or light reflections on the surface.\n\nGreen-glass misclassified as Brown-glass: 7 cases (7.4% of the green-glass class)\nThis is a fairly understandable error, as both are colored glass and only differ in shade. Under varying lighting conditions, distinguishing between them can be difficult.\n\nPaper misclassified as Cardboard or Plastic: 7 cases each (4.4% of the paper class)\nPaper and cardboard share similar components (cellulose fibers).\nSome glossy paper types might be mistaken for plastic due to their appearance.\n\nPaper misclassified as Battery: 5 cases (3.2% of the paper class)\nThis is an interesting and less intuitive error.\nIt might be caused by certain battery packaging being made of paper, or images of batteries printed on paper.","metadata":{}},{"cell_type":"code","source":"# Create image upload widget and predict result\nfrom IPython.display import display\nimport ipywidgets as widgets\nimport io\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.xception import preprocess_input\n\ndef create_test_widget():\n    # File upload widget\n    uploader = widgets.FileUpload(\n        accept='image/*',\n        multiple=False,\n        description='Upload image:',\n        button_style='primary'\n    )\n    \n    # Output widget to display results\n    output = widgets.Output()\n    \n    # Handler function when a file is uploaded\n    def on_upload_change(change):\n        with output:\n            output.clear_output()\n            \n            if uploader.value:\n                # Get image file\n                file_info = next(iter(uploader.value.values()))\n                file_name = file_info['name']\n                img_bytes = file_info['content']\n                \n                print(f\"Uploaded: {file_name}\")\n                \n                # Load and display image\n                img = image.load_img(io.BytesIO(img_bytes), target_size=IMAGE_SIZE)\n                \n                plt.figure(figsize=(8, 8))\n                plt.imshow(img)\n                plt.axis('off')\n                plt.title(\"Uploaded image\", fontsize=14)\n                plt.show()\n                \n                # Preprocess image\n                x = image.img_to_array(img)\n                x = np.expand_dims(x, axis=0)\n                x = preprocess_input(x)\n                \n                # Predict\n                preds = model.predict(x, verbose=0)\n                \n                # Get top 3 classes with highest probabilities\n                top_3_indices = np.argsort(preds[0])[-3:][::-1]\n                top_3_classes = [class_indices_inv[idx] for idx in top_3_indices]\n                top_3_probs = [preds[0][idx] for idx in top_3_indices]\n                \n                # Display results\n                print(\"\\n📊 Prediction results:\")\n                print(\"=\" * 40)\n                for i, (cls, prob) in enumerate(zip(top_3_classes, top_3_probs)):\n                    print(f\"{i+1}. {cls.upper()}: {prob*100:.2f}%\")\n                print(\"=\" * 40)\n                \n                # Plot probability bar chart for top classes\n                plt.figure(figsize=(10, 6))\n                colors = ['#2C8ECF', '#8EB33B', '#D3A63A'] if len(top_3_classes) >= 3 else ['#2C8ECF', '#8EB33B']\n                bars = plt.barh(top_3_classes, top_3_probs, color=colors)\n                plt.xlabel('Probability', fontsize=12)\n                plt.title('Top 3 trash categories with highest probability', fontsize=14)\n                plt.xlim(0, 1)\n                plt.grid(axis='x', linestyle='--', alpha=0.6)\n                \n                # Add value labels to bars\n                for bar, prob in zip(bars, top_3_probs):\n                    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n                            f'{prob*100:.1f}%', va='center', fontsize=12)\n                \n                plt.tight_layout()\n                plt.show()\n                \n                # Show advice based on confidence\n                if top_3_probs[0] > 0.9:\n                    print(f\"✅ Clear classification: This is {top_3_classes[0]} with high confidence.\")\n                elif top_3_probs[0] > 0.7:\n                    print(f\"🟢 Fairly confident classification: Seems to be {top_3_classes[0]}.\")\n                elif top_3_probs[0] - top_3_probs[1] < 0.1:\n                    print(f\"⚠️ Unclear classification: Could be {top_3_classes[0]} or {top_3_classes[1]}.\")\n                    print(f\"   The difference between these two classes is only {(top_3_probs[0] - top_3_probs[1])*100:.1f}%.\")\n                else:\n                    print(f\"🟡 Might be {top_3_classes[0]}, but not completely certain.\")\n    \n    # Register handler\n    uploader.observe(on_upload_change, names='value')\n    \n    # Display widget\n    print(\"📷 Upload trash image for classification:\")\n    display(uploader)\n    display(output)\n\n# Create widget to test model with new images\ncreate_test_widget()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:05:35.766703Z","iopub.execute_input":"2025-05-21T14:05:35.766926Z","iopub.status.idle":"2025-05-21T14:05:35.789091Z","shell.execute_reply.started":"2025-05-21T14:05:35.766910Z","shell.execute_reply":"2025-05-21T14:05:35.788430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if the saved model files exist\nimport os\n\nmodel_files = [\n    \"garbage_classification_model.keras\",\n    \"best_garbage_classification_model.keras\",\n    \"fine_tuned_garbage_model.keras\",\n    \"fine_tuned_model.keras\",\n    \"continued_fine_tuned_model.keras\"\n]\n\nprint(\"Checking saved model files:\")\nfor file in model_files:\n    if os.path.exists(file):\n        file_size = os.path.getsize(file) / (1024 * 1024)  # Convert to MB\n        print(f\"✅ {file} - Size: {file_size:.2f} MB\")\n    else:\n        print(f\"❌ {file} - Not found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:05:35.789897Z","iopub.execute_input":"2025-05-21T14:05:35.790644Z","iopub.status.idle":"2025-05-21T14:05:35.795109Z","shell.execute_reply.started":"2025-05-21T14:05:35.790623Z","shell.execute_reply":"2025-05-21T14:05:35.794613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check entire working directory for model files\nimport os\n\ndef find_model_files(directory='.'):\n    model_files = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.keras') or file.endswith('.h5'):\n                model_files.append(os.path.join(root, file))\n    return model_files\n\nmodel_files = find_model_files()\nif model_files:\n    print(\"Found the following model files:\")\n    for file in model_files:\n        file_size = os.path.getsize(file) / (1024 * 1024)  # Convert to MB\n        print(f\"- {file} - Size: {file_size:.2f} MB\")\nelse:\n    print(\"No model files found in the workspace.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:05:35.798127Z","iopub.execute_input":"2025-05-21T14:05:35.798436Z","iopub.status.idle":"2025-05-21T14:05:35.809987Z","shell.execute_reply.started":"2025-05-21T14:05:35.798385Z","shell.execute_reply":"2025-05-21T14:05:35.809139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the current model to the /kaggle/working/ directory\nsave_path = '/kaggle/working/current_model.keras'\nmodel.save(save_path)\nprint(f\"Model saved to {save_path}\")\n\n# Check if the file has been saved\nif os.path.exists(save_path):\n    file_size = os.path.getsize(save_path) / (1024 * 1024)  # Convert to MB\n    print(f\"Saved successfully! File size: {file_size:.2f} MB\")\nelse:\n    print(\"Model saving failed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:05:35.810761Z","iopub.execute_input":"2025-05-21T14:05:35.810945Z","iopub.status.idle":"2025-05-21T14:05:36.601170Z","shell.execute_reply.started":"2025-05-21T14:05:35.810922Z","shell.execute_reply":"2025-05-21T14:05:36.600407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}